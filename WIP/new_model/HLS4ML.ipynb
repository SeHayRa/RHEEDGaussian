{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import h5py\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 21:24:10.567533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 21:24:11.321746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "['cat', 'concat', 'concatenate', 'add', 'mul', 'multiply', 'sub', 'subtract', 'fmin', 'minimum', 'fmax', 'maximum', 'Linear', 'Softmax', 'ReLU', 'LeakyReLU', 'Threshold', 'ELU', 'PReLU', 'Sigmoid', 'Tanh', 'BatchNorm2d', 'BatchNorm1d', 'Batch_norm', 'Flatten', 'View', 'MaxPool1d', 'MaxPool2d', 'AvgPool1d', 'AvgPool2d', 'Conv1d', 'Conv2d']\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "test = hls4ml.converters.get_supported_pytorch_layers()\n",
    "print('MaxPool2d' in test)\n",
    "print('MaxPool1d' in test)\n",
    "\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "     def __init__(self,num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 4, stride = 4))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(144, 48)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(48, 24)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(24, num_classes)\n",
    "        self.flatten=nn.Flatten()\n",
    "\n",
    "     def forward(self, x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "       \n",
    "        out=self.flatten(out)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        out = self.relu1(out)\n",
    "        embedding = self.fc2(out)\n",
    "       \n",
    "        \n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=144, out_features=48, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=48, out_features=24, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=24, out_features=5, bias=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pytorch=LeNet5(5)\n",
    "model_pytorch.load_state_dict(torch.load('PytorchLeNet_4848_new.pt'))\n",
    "model_pytorch.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 48, 48])\n",
      "tensor([[  0.3999,  -7.2148,  -1.5861,  -2.9926, -10.6000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is the scaled output, the proper math needs to be done to unscale it '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/ferroelectric/sean/RHEED_4848_test6.h5'\n",
    "\n",
    "with h5py.File(path, 'r') as h5:\n",
    "    img = h5['growth_1']['spot_2'][1]\n",
    "    img= img/img.max()\n",
    "img_tensor = torch.tensor(img).unsqueeze(0).unsqueeze(0).float()\n",
    "print(img_tensor.shape)\n",
    "with torch.no_grad():\n",
    "    output_pytorch = model_pytorch(img_tensor)\n",
    "print(output_pytorch)\n",
    "\n",
    "\"\"\"This is the scaled output, the proper math needs to be done to unscale it \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "  Precision:         ap_fixed<16,6>\n",
      "  ReuseFactor:       2\n",
      "  InputsChannelLast: True\n",
      "  TransposeOutputs:  False\n",
      "  Strategy:          Latency\n",
      "LayerName\n",
      "  fc\n",
      "    Strategy:        Resource\n",
      "    ReuseFactor:     128\n",
      "  fc1\n",
      "    Strategy:        Resource\n",
      "    ReuseFactor:     128\n",
      "  fc2\n",
      "    Strategy:        Resource\n",
      "    ReuseFactor:     64\n",
      "Interpreting Model ...\n",
      "Topology:\n",
      "Layer name: layer1_0, layer type: Conv2D, input shape: [[None, 1, 48, 48]]\n",
      "Layer name: layer1_1, layer type: BatchNormalization, input shape: [[None, 6, 44, 44]]\n",
      "Layer name: layer1_2, layer type: Activation, input shape: [[None, 6, 44, 44]]\n",
      "Layer name: layer1_3, layer type: MaxPooling2D, input shape: [[None, 6, 44, 44]]\n",
      "Layer name: layer2_0, layer type: Conv2D, input shape: [[None, 6, 11, 11]]\n",
      "Layer name: layer2_1, layer type: BatchNormalization, input shape: [[None, 16, 7, 7]]\n",
      "Layer name: layer2_2, layer type: Activation, input shape: [[None, 16, 7, 7]]\n",
      "Layer name: layer2_3, layer type: MaxPooling2D, input shape: [[None, 16, 7, 7]]\n",
      "Layer name: flatten, layer type: Reshape, input shape: [[None, 16, 3, 3]]\n",
      "Layer name: fc, layer type: Dense, input shape: [[None, 144]]\n",
      "Layer name: relu, layer type: Activation, input shape: [[None, 48]]\n",
      "Layer name: fc1, layer type: Dense, input shape: [[None, 48]]\n",
      "Layer name: relu1, layer type: Activation, input shape: [[None, 24]]\n",
      "Layer name: fc2, layer type: Dense, input shape: [[None, 24]]\n",
      "Creating HLS model\n",
      "WARNING: Strategy for layer fc set to \"Resource\", while pipeline style set to \"pipeline\".\n",
      "WARNING: Strategy for layer fc1 set to \"Resource\", while pipeline style set to \"pipeline\".\n",
      "WARNING: Strategy for layer fc2 set to \"Resource\", while pipeline style set to \"pipeline\".\n",
      "WARNING: Changing pipeline style to \"dataflow\".\n",
      "WARNING: Invalid ReuseFactor=128 in layer \"fc\".Using ReuseFactor=144 instead. Valid ReuseFactor(s): 1,2,3,4,6,8,9,12,16,18,24,36,48,72,144,288,432,576,864,1152,1728,2304,3456,6912.\n",
      "WARNING: Invalid ReuseFactor=128 in layer \"fc1\".Using ReuseFactor=144 instead. Valid ReuseFactor(s): 1,2,3,4,6,8,12,16,24,48,96,144,192,288,384,576,1152.\n",
      "WARNING: Invalid ReuseFactor=64 in layer \"fc2\".Using ReuseFactor=24 instead. Valid ReuseFactor(s): 1,2,3,4,6,8,12,24,120.\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#changing around reuse values\n",
    "\n",
    "def print_dict(d, indent=0):\n",
    "    align = 20\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key), end='')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_dict(value, indent+1)\n",
    "        else:\n",
    "            print(':' + ' ' * (20 - len(key) - 2 * indent) + str(value))  \n",
    "\n",
    "\n",
    "config = hls4ml.utils.config_from_pytorch_model(model_pytorch, inputs_channel_last=True,transpose_outputs=False,default_reuse_factor=2, granularity = \"name\")\n",
    "\n",
    "config[\"LayerName\"] = {}\n",
    "config[\"LayerName\"][\"fc\"] = {}\n",
    "config[\"LayerName\"][\"fc1\"] = {}\n",
    "config[\"LayerName\"][\"fc2\"] = {}\n",
    "\n",
    "config[\"LayerName\"][\"fc\"][\"Strategy\"] = \"Resource\"\n",
    "config[\"LayerName\"][\"fc\"][\"ReuseFactor\"] = 128\n",
    "config[\"LayerName\"][\"fc1\"][\"Strategy\"] = \"Resource\"\n",
    "config[\"LayerName\"][\"fc1\"][\"ReuseFactor\"] = 128\n",
    "config[\"LayerName\"][\"fc2\"][\"Strategy\"] = \"Resource\"\n",
    "config[\"LayerName\"][\"fc2\"][\"ReuseFactor\"] = 64\n",
    "\n",
    "    \n",
    "print_dict(config)\n",
    "\n",
    "model_hls = hls4ml.converters.convert_from_pytorch_model(model_pytorch, [[None,1,48,48]], hls_config=config, output_dir='hls_rheed_4848_new', project_name='rheed', part='xcku035-fbva676-2-e', io_type='io_stream' )\n",
    "model_hls.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
